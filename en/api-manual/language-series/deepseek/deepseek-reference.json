{
  "openapi": "3.1.0",
  "info": {
    "title": "DeepSeek Complete API Reference",
    "description": "Complete API reference for DeepSeek chat interface, including all parameters and advanced features",
    "license": {
      "name": "MIT"
    },
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "https://api.evolink.ai",
      "description": "Production environment"
    }
  ],
  "security": [
    {
      "bearerAuth": []
    }
  ],
  "paths": {
    "/v1/chat/completions": {
      "post": {
        "summary": "DeepSeek Chat API",
        "description": "- Call DeepSeek models using OpenAI SDK format\n- Synchronous processing mode, real-time response\n- Supports `deepseek-chat` (general conversation) and `deepseek-reasoner` (deep reasoning) models\n- **Text Chat**: Single or multi-turn contextual conversation\n- **System Prompts**: Customize AI role and behavior\n- **Streaming**: SSE streaming output support\n- **Tool Calling**: Function Calling support",
        "operationId": "createChatCompletionDeepSeek",
        "tags": [
          "Chat Completion"
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/ChatCompletionRequest"
              },
              "examples": {
                "simple_text": {
                  "summary": "Simple text chat",
                  "value": {
                    "model": "deepseek-chat",
                    "messages": [
                      {
                        "role": "user",
                        "content": "Tell me about yourself"
                      }
                    ]
                  }
                },
                "multi_turn": {
                  "summary": "Multi-turn conversation",
                  "value": {
                    "model": "deepseek-chat",
                    "messages": [
                      {
                        "role": "user",
                        "content": "What is Python?"
                      },
                      {
                        "role": "assistant",
                        "content": "Python is a high-level programming language..."
                      },
                      {
                        "role": "user",
                        "content": "What are its advantages?"
                      }
                    ]
                  }
                },
                "system_prompt": {
                  "summary": "Using system prompt",
                  "value": {
                    "model": "deepseek-chat",
                    "messages": [
                      {
                        "role": "system",
                        "content": "You are a professional Python programming assistant. Answer questions concisely."
                      },
                      {
                        "role": "user",
                        "content": "How to read a file?"
                      }
                    ]
                  }
                },
                "reasoner": {
                  "summary": "Using deep reasoning model",
                  "value": {
                    "model": "deepseek-reasoner",
                    "messages": [
                      {
                        "role": "user",
                        "content": "Prove that the square root of 2 is irrational"
                      }
                    ]
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Chat completion generated successfully",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ChatCompletionResponse"
                }
              }
            }
          },
          "400": {
            "description": "Invalid request parameters",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 400,
                    "message": "Invalid request parameters",
                    "type": "invalid_request_error"
                  }
                }
              }
            }
          },
          "401": {
            "description": "Unauthenticated, invalid or expired token",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 401,
                    "message": "Invalid or expired token",
                    "type": "authentication_error"
                  }
                }
              }
            }
          },
          "402": {
            "description": "Insufficient quota, recharge required",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 402,
                    "message": "Insufficient quota",
                    "type": "insufficient_quota_error"
                  }
                }
              }
            }
          },
          "403": {
            "description": "Access denied",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 403,
                    "message": "Access denied for this model",
                    "type": "permission_error",
                    "param": "model"
                  }
                }
              }
            }
          },
          "404": {
            "description": "Resource not found",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 404,
                    "message": "Specified model not found",
                    "type": "not_found_error",
                    "param": "model"
                  }
                }
              }
            }
          },
          "413": {
            "description": "Request body too large",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 413,
                    "message": "Request body too large",
                    "type": "request_too_large_error",
                    "param": "messages"
                  }
                }
              }
            }
          },
          "429": {
            "description": "Rate limit exceeded",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 429,
                    "message": "Rate limit exceeded",
                    "type": "rate_limit_error"
                  }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 500,
                    "message": "Internal server error",
                    "type": "internal_server_error"
                  }
                }
              }
            }
          },
          "502": {
            "description": "Upstream service error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 502,
                    "message": "Upstream AI service unavailable",
                    "type": "upstream_error"
                  }
                }
              }
            }
          },
          "503": {
            "description": "Service temporarily unavailable",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                },
                "example": {
                  "error": {
                    "code": 503,
                    "message": "Service temporarily unavailable",
                    "type": "service_unavailable_error"
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "ChatCompletionRequest": {
        "type": "object",
        "required": [
          "model",
          "messages"
        ],
        "properties": {
          "model": {
            "type": "string",
            "description": "Chat model name\n\n- `deepseek-chat`: General conversation model\n- `deepseek-reasoner`: Deep reasoning model, excels at math, coding and complex logical reasoning\n\n**Note**: `deepseek-reasoner` does not support `temperature`, `top_p`, `tools`, `tool_choice`, `response_format` parameters. Passing these will be rejected by upstream",
            "enum": [
              "deepseek-chat",
              "deepseek-reasoner"
            ],
            "default": "deepseek-chat",
            "example": "deepseek-chat"
          },
          "messages": {
            "type": "array",
            "description": "Conversation message list, supports multi-turn conversation\n\nDifferent roles have different field structures, select the corresponding role to view",
            "items": {
              "oneOf": [
                {
                  "$ref": "#/components/schemas/SystemMessage"
                },
                {
                  "$ref": "#/components/schemas/UserMessage"
                },
                {
                  "$ref": "#/components/schemas/AssistantRequestMessage"
                },
                {
                  "$ref": "#/components/schemas/ToolMessage"
                }
              ],
              "discriminator": {
                "propertyName": "role",
                "mapping": {
                  "system": "#/components/schemas/SystemMessage",
                  "user": "#/components/schemas/UserMessage",
                  "assistant": "#/components/schemas/AssistantRequestMessage",
                  "tool": "#/components/schemas/ToolMessage"
                }
              }
            },
            "minItems": 1
          },
          "thinking": {
            "type": "object",
            "description": "Thinking mode control (Beta)\n\n**Details**:\n- Controls the deep thinking feature of `deepseek-reasoner` model\n- When enabled, the model will perform deep reasoning before responding",
            "properties": {
              "type": {
                "type": "string",
                "description": "Thinking mode switch\n\n- `enabled`: Enable deep thinking\n- `disabled`: Disable deep thinking",
                "enum": [
                  "enabled",
                  "disabled"
                ]
              }
            }
          },
          "frequency_penalty": {
            "type": "number",
            "description": "Frequency penalty parameter to reduce repetitive content\n\n**Details**:\n- Positive values penalize tokens based on their frequency in the generated text\n- Higher values make it less likely to repeat existing content\n- Default: 0 (no penalty)",
            "minimum": -2,
            "maximum": 2,
            "default": 0,
            "example": 0
          },
          "max_tokens": {
            "type": "integer",
            "description": "Maximum number of tokens to generate\n\n**Details**:\n- The model will stop generating when this limit is reached\n- If not set, the model decides the generation length",
            "minimum": 1,
            "example": 4096
          },
          "presence_penalty": {
            "type": "number",
            "description": "Presence penalty parameter to encourage new topics\n\n**Details**:\n- Positive values penalize tokens based on whether they have appeared in the text\n- Higher values encourage discussing new topics\n- Default: 0 (no penalty)",
            "minimum": -2,
            "maximum": 2,
            "default": 0,
            "example": 0
          },
          "response_format": {
            "type": "object",
            "description": "Specify response format\n\n**Details**:\n- Set to `{\"type\": \"json_object\"}` to enable JSON mode\n- In JSON mode, the model will output valid JSON content",
            "properties": {
              "type": {
                "type": "string",
                "enum": [
                  "text",
                  "json_object"
                ],
                "description": "Response format type",
                "default": "text"
              }
            }
          },
          "stop": {
            "description": "Stop sequences. The model will stop generating when encountering these strings\n\n**Details**:\n- Can be a single string or an array of strings\n- Maximum 16 stop sequences",
            "oneOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "maxItems": 16
              }
            ]
          },
          "stream": {
            "type": "boolean",
            "description": "Whether to stream the response\n\n- `true`: Stream via SSE (Server-Sent Events), returning content in real-time chunks\n- `false`: Wait for the complete response before returning",
            "default": false,
            "example": false
          },
          "stream_options": {
            "type": "object",
            "description": "Streaming response options\n\nOnly effective when `stream=true`",
            "properties": {
              "include_usage": {
                "type": "boolean",
                "description": "Return usage statistics at the end of the stream"
              }
            }
          },
          "temperature": {
            "type": "number",
            "description": "Sampling temperature, controls output randomness\n\n**Details**:\n- Lower values (e.g. 0.2): More deterministic, focused output\n- Higher values (e.g. 1.5): More random, creative output\n- Default: 1",
            "minimum": 0,
            "maximum": 2,
            "default": 1,
            "example": 1
          },
          "top_p": {
            "type": "number",
            "description": "Nucleus Sampling parameter\n\n**Details**:\n- Controls sampling from tokens whose cumulative probability reaches the threshold\n- For example, 0.9 means sampling from tokens reaching 90% cumulative probability\n- Default: 1.0 (consider all tokens)\n\n**Tip**: Avoid adjusting both temperature and top_p simultaneously",
            "minimum": 0,
            "maximum": 1,
            "default": 1,
            "example": 1
          },
          "tools": {
            "type": "array",
            "description": "Tool definition list for Function Calling\n\n**Details**:\n- Maximum 128 tool definitions\n- Each tool requires a name, description and parameter schema",
            "items": {
              "$ref": "#/components/schemas/Tool"
            },
            "maxItems": 128
          },
          "tool_choice": {
            "description": "Controls tool calling behavior\n\n**Options**:\n- `none`: Do not call any tools\n- `auto`: Model decides whether to call tools\n- `required`: Force the model to call one or more tools\n\n**Default**: `none` when no tools provided, `auto` when tools are provided",
            "oneOf": [
              {
                "type": "string",
                "enum": [
                  "none",
                  "auto",
                  "required"
                ]
              },
              {
                "type": "object",
                "description": "Specify a particular tool to call",
                "properties": {
                  "type": {
                    "type": "string",
                    "enum": [
                      "function"
                    ]
                  },
                  "function": {
                    "type": "object",
                    "properties": {
                      "name": {
                        "type": "string",
                        "description": "Name of the function to call"
                      }
                    },
                    "required": [
                      "name"
                    ]
                  }
                }
              }
            ]
          },
          "logprobs": {
            "type": "boolean",
            "description": "Whether to return token log probabilities\n\n**Details**:\n- When set to `true`, the response will include log probability information for each token",
            "default": false
          },
          "top_logprobs": {
            "type": "integer",
            "description": "Return log probabilities of the top N most likely tokens\n\n**Details**:\n- Requires `logprobs` to be set to `true`\n- Range: `[0, 20]`",
            "minimum": 0,
            "maximum": 20
          }
        }
      },
      "SystemMessage": {
        "title": "System Message",
        "type": "object",
        "required": ["role", "content"],
        "properties": {
          "role": {
            "type": "string",
            "enum": ["system"],
            "description": "Role identifier, fixed as `system`"
          },
          "content": {
            "type": "string",
            "description": "System prompt content, used to define AI role and behavior"
          },
          "name": {
            "type": "string",
            "description": "Participant name, used to distinguish different system prompt sources"
          }
        }
      },
      "UserMessage": {
        "title": "User Message",
        "type": "object",
        "required": ["role", "content"],
        "properties": {
          "role": {
            "type": "string",
            "enum": ["user"],
            "description": "Role identifier, fixed as `user`"
          },
          "content": {
            "type": "string",
            "description": "User message content (plain text string)"
          },
          "name": {
            "type": "string",
            "description": "Participant name, used to distinguish different users"
          }
        }
      },
      "AssistantRequestMessage": {
        "title": "Assistant Message",
        "type": "object",
        "required": ["role", "content"],
        "properties": {
          "role": {
            "type": "string",
            "enum": ["assistant"],
            "description": "Role identifier, fixed as `assistant`"
          },
          "content": {
            "type": ["string", "null"],
            "description": "Assistant message content\n\n**Details**:\n- Used to pass historical assistant replies in multi-turn conversations\n- Can be `null` when `tool_calls` is present"
          },
          "name": {
            "type": "string",
            "description": "Participant name"
          },
          "prefix": {
            "type": "boolean",
            "description": "Enable prefix continuation mode (Beta)\n\n**Details**:\n- Only on the last message\n- When `true`, the model continues generating from this message's `content` as prefix",
            "default": false
          },
          "reasoning_content": {
            "type": ["string", "null"],
            "description": "Chain of Thought content (Beta)\n\n**Details**:\n- Only effective when using `deepseek-reasoner` model\n- Used to pass historical reasoning process in multi-turn conversations\n- Requires `prefix` to be set to `true`"
          },
          "tool_calls": {
            "type": "array",
            "description": "Tool call list\n\nUsed to pass historical tool call information in multi-turn conversations",
            "items": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string",
                  "description": "Unique identifier for the tool call"
                },
                "type": {
                  "type": "string",
                  "enum": ["function"]
                },
                "function": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "Name of the called function"
                    },
                    "arguments": {
                      "type": "string",
                      "description": "Function arguments (JSON string)"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "ToolMessage": {
        "title": "Tool Message",
        "type": "object",
        "required": ["role", "content", "tool_call_id"],
        "properties": {
          "role": {
            "type": "string",
            "enum": ["tool"],
            "description": "Role identifier, fixed as `tool`"
          },
          "content": {
            "type": "string",
            "description": "Tool call result content"
          },
          "tool_call_id": {
            "type": "string",
            "description": "Tool call ID\n\nCorresponds to the `id` field returned in assistant message's `tool_calls`"
          }
        }
      },
      "Tool": {
        "type": "object",
        "required": [
          "type",
          "function"
        ],
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "function"
            ],
            "description": "Tool type, currently only `function` is supported"
          },
          "function": {
            "type": "object",
            "required": [
              "name"
            ],
            "properties": {
              "name": {
                "type": "string",
                "description": "Name of the function to call\n\n**Details**:\n- Must consist of a-z, A-Z, 0-9 characters, or contain underscores and hyphens\n- Maximum length of 64 characters"
              },
              "description": {
                "type": "string",
                "description": "Function description, helps the model understand when and how to call this function"
              },
              "parameters": {
                "type": "object",
                "description": "Function input parameters, described as a JSON Schema object\n\n**Details**:\n- Omitting `parameters` defines a function with an empty parameter list"
              },
              "strict": {
                "type": "boolean",
                "description": "Enable strict mode (Beta)\n\n**Details**:\n- When set to `true`, the API will use strict mode for function calls\n- Ensures output always conforms to the function's JSON Schema definition",
                "default": false
              }
            }
          }
        }
      },
      "ChatCompletionResponse": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string",
            "description": "Unique identifier for the chat completion",
            "example": "930c60df-bf64-41c9-a88e-3ec75f81e00e"
          },
          "model": {
            "type": "string",
            "description": "Actual model name used",
            "example": "deepseek-chat"
          },
          "object": {
            "type": "string",
            "enum": [
              "chat.completion"
            ],
            "description": "Response type",
            "example": "chat.completion"
          },
          "created": {
            "type": "integer",
            "description": "Creation timestamp",
            "example": 1770617860
          },
          "choices": {
            "type": "array",
            "description": "List of chat completion choices",
            "items": {
              "$ref": "#/components/schemas/Choice"
            }
          },
          "usage": {
            "$ref": "#/components/schemas/Usage"
          },
          "system_fingerprint": {
            "type": "string",
            "description": "System fingerprint identifier",
            "example": "fp_eaab8d114b_prod0820_fp8_kvcache"
          }
        }
      },
      "Choice": {
        "type": "object",
        "properties": {
          "index": {
            "type": "integer",
            "description": "Choice index",
            "example": 0
          },
          "message": {
            "$ref": "#/components/schemas/AssistantMessage"
          },
          "finish_reason": {
            "type": "string",
            "description": "Finish reason\n\n- `stop`: Natural completion or stop sequence reached\n- `length`: Max token limit reached\n- `content_filter`: Output filtered by safety policy\n- `tool_calls`: Model called a tool\n- `insufficient_system_resource`: Backend resource constraints",
            "enum": [
              "stop",
              "length",
              "content_filter",
              "tool_calls",
              "insufficient_system_resource"
            ],
            "example": "stop"
          }
        }
      },
      "AssistantMessage": {
        "type": "object",
        "properties": {
          "role": {
            "type": "string",
            "description": "Role of the message sender",
            "enum": [
              "assistant"
            ],
            "example": "assistant"
          },
          "content": {
            "type": "string",
            "description": "AI response content",
            "example": "Hello! I'm DeepSeek, a powerful AI assistant. I excel at general conversation, code generation, mathematical reasoning and many other tasks."
          },
          "reasoning_content": {
            "type": "string",
            "description": "Reasoning process content (only returned by `deepseek-reasoner` model)\n\n**Details**:\n- Contains the model's Chain of Thought process\n- Only returned when using the `deepseek-reasoner` model",
            "example": "Let me analyze this problem..."
          },
          "tool_calls": {
            "type": "array",
            "description": "Tool call list (returned when the model decides to call a tool)",
            "items": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string",
                  "description": "Unique identifier for the tool call"
                },
                "type": {
                  "type": "string",
                  "enum": [
                    "function"
                  ]
                },
                "function": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "Name of the called function"
                    },
                    "arguments": {
                      "type": "string",
                      "description": "Function arguments (JSON string)"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "Usage": {
        "type": "object",
        "description": "Token usage statistics",
        "properties": {
          "prompt_tokens": {
            "type": "integer",
            "description": "Number of tokens in the input",
            "example": 16
          },
          "completion_tokens": {
            "type": "integer",
            "description": "Number of tokens in the output",
            "example": 10
          },
          "total_tokens": {
            "type": "integer",
            "description": "Total number of tokens",
            "example": 26
          },
          "prompt_cache_hit_tokens": {
            "type": "integer",
            "description": "Number of cache-hit tokens in the input",
            "example": 0
          },
          "prompt_cache_miss_tokens": {
            "type": "integer",
            "description": "Number of cache-miss tokens in the input",
            "example": 16
          }
        }
      },
      "ErrorResponse": {
        "type": "object",
        "properties": {
          "error": {
            "type": "object",
            "properties": {
              "code": {
                "type": "integer",
                "description": "HTTP status error code"
              },
              "message": {
                "type": "string",
                "description": "Error description"
              },
              "type": {
                "type": "string",
                "description": "Error type"
              },
              "param": {
                "type": "string",
                "description": "Related parameter name"
              }
            }
          }
        }
      }
    },
    "securitySchemes": {
      "bearerAuth": {
        "type": "http",
        "scheme": "bearer",
        "description": "##All APIs require Bearer Token authentication##\n\n**Get API Key:**\n\nVisit [API Key Management Page](https://evolink.ai/dashboard/keys) to get your API Key\n\n**Add to request header:**\n```\nAuthorization: Bearer YOUR_API_KEY\n```"
      }
    }
  },
  "tags": [
    {
      "name": "Chat Completion",
      "description": "AI chat completion related APIs"
    }
  ]
}
