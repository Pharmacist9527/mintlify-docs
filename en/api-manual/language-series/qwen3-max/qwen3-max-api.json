{
  "openapi": "3.1.0",
  "info": {
    "title": "Qwen3-Max Complete API Reference",
    "description": "Complete API reference for Qwen3-Max chat interface, including all parameters and advanced features",
    "license": {
      "name": "MIT"
    },
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "https://api.evolink.ai",
      "description": "Production environment"
    }
  ],
  "security": [
    {
      "bearerAuth": []
    }
  ],
  "paths": {
    "/v1/chat/completions": {
      "post": {
        "summary": "Qwen3-Max Chat Interface",
        "description": "- Use OpenAI SDK format to call Qwen3-Max model\n- Synchronous processing mode, real-time response\n- **Text conversation**: Single or multi-turn contextual dialogue, see simple_text and multi_turn examples\n- **System prompts**: Customize AI role and behavior, see system_prompt example\n- **Thinking mode**: Supports thinking mode, reasoning content returned via reasoning_content, see thinking example\n- **Web search**: Supports web search to enhance answers, see web_search example\n- **Tool calling**: Supports Function Calling, see tool_use example\n- **JSON output**: Supports json_object and json_schema format output",
        "operationId": "createChatCompletion",
        "tags": [
          "Chat Completion"
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/ChatCompletionRequest"
              },
              "examples": {
                "simple_text": {
                  "summary": "Single-turn text conversation",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "user",
                        "content": "Please introduce yourself"
                      }
                    ],
                    "temperature": 0.7
                  }
                },
                "multi_turn": {
                  "summary": "Multi-turn conversation (context understanding)",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "user",
                        "content": "What is quantum computing?"
                      },
                      {
                        "role": "assistant",
                        "content": "Quantum computing is a technology that uses quantum mechanics principles..."
                      },
                      {
                        "role": "user",
                        "content": "How is it different from traditional computing?"
                      }
                    ],
                    "temperature": 0.7
                  }
                },
                "system_prompt": {
                  "summary": "Using system prompts",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "system",
                        "content": "You are a professional programming assistant, skilled at answering various programming questions with concise and accurate responses."
                      },
                      {
                        "role": "user",
                        "content": "What is the difference between list and tuple in Python?"
                      }
                    ],
                    "temperature": 0.7
                  }
                },
                "thinking": {
                  "summary": "Thinking mode",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "user",
                        "content": "How many r's are in the word strawberry?"
                      }
                    ],
                    "stream": true,
                    "enable_thinking": true,
                    "temperature": 0.7
                  }
                },
                "web_search": {
                  "summary": "Web search",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "user",
                        "content": "What are today's top tech news?"
                      }
                    ],
                    "enable_search": true,
                    "search_options": {
                      "search_strategy": "turbo"
                    },
                    "temperature": 0.7
                  }
                },
                "tool_use": {
                  "summary": "Tool calling (Function Calling)",
                  "value": {
                    "model": "qwen3-max-2026-01-23",
                    "messages": [
                      {
                        "role": "user",
                        "content": "What's the weather like in Beijing today?"
                      }
                    ],
                    "tools": [
                      {
                        "type": "function",
                        "function": {
                          "name": "get_weather",
                          "description": "Get weather information for a specified city",
                          "parameters": {
                            "type": "object",
                            "properties": {
                              "city": {
                                "type": "string",
                                "description": "City name"
                              }
                            },
                            "required": ["city"]
                          }
                        }
                      }
                    ],
                    "temperature": 0.7
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Chat completion successful",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ChatCompletionResponse"
                }
              }
            }
          },
          "400": {
            "description": "Invalid request parameters",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 400, "message": "Invalid request parameters", "type": "invalid_request_error" }
                }
              }
            }
          },
          "401": {
            "description": "Unauthorized, invalid or expired token",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 401, "message": "Invalid or expired token", "type": "authentication_error" }
                }
              }
            }
          },
          "402": {
            "description": "Insufficient quota, recharge required",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 402, "message": "Insufficient quota", "type": "insufficient_quota_error", "fallback_suggestion": "https://evolink.ai/dashboard/billing" }
                }
              }
            }
          },
          "403": {
            "description": "Access denied",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 403, "message": "Access denied for this model", "type": "permission_error", "param": "model" }
                }
              }
            }
          },
          "404": {
            "description": "Resource not found",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 404, "message": "Specified model not found", "type": "not_found_error", "param": "model", "fallback_suggestion": "qwen3-max-2026-01-23" }
                }
              }
            }
          },
          "413": {
            "description": "Request payload too large",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 413, "message": "Request payload too large", "type": "request_too_large_error", "param": "content" }
                }
              }
            }
          },
          "429": {
            "description": "Rate limit exceeded",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 429, "message": "Rate limit exceeded", "type": "rate_limit_error", "fallback_suggestion": "retry after 60 seconds" }
                }
              }
            }
          },
          "500": {
            "description": "Internal server error",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 500, "message": "Internal server error", "type": "internal_server_error", "fallback_suggestion": "try again later" }
                }
              }
            }
          },
          "502": {
            "description": "Upstream service error",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 502, "message": "Upstream AI service unavailable", "type": "upstream_error", "fallback_suggestion": "try different model" }
                }
              }
            }
          },
          "503": {
            "description": "Service temporarily unavailable",
            "content": {
              "application/json": {
                "schema": { "$ref": "#/components/schemas/ErrorResponse" },
                "example": {
                  "error": { "code": 503, "message": "Service temporarily unavailable", "type": "service_unavailable_error", "fallback_suggestion": "retry after 30 seconds" }
                }
              }
            }
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "ChatCompletionRequest": {
        "type": "object",
        "required": ["model", "messages"],
        "properties": {
          "model": {
            "type": "string",
            "description": "Model name for chat completion",
            "enum": ["qwen3-max-2026-01-23"],
            "example": "qwen3-max-2026-01-23"
          },
          "messages": {
            "type": "array",
            "description": "List of messages for the conversation, supports system/user/assistant/tool roles",
            "items": { "$ref": "#/components/schemas/Message" },
            "minItems": 1
          },
          "stream": {
            "type": "boolean",
            "description": "Whether to stream the response\n\n- `true`: Stream response, returns content chunk by chunk in real-time\n- `false`: Wait for complete response and return all at once",
            "default": false,
            "example": false
          },
          "stream_options": {
            "type": "object",
            "description": "Stream output options, only effective when stream=true",
            "properties": {
              "include_usage": {
                "type": "boolean",
                "description": "Whether the last chunk includes token usage info\n\n**Note**:\n- Only effective when stream=true\n- When set to true, the last chunk will include the usage field",
                "default": false
              }
            }
          },
          "temperature": {
            "type": "number",
            "description": "Sampling temperature, controls randomness of output\n\n**Note**:\n- Lower values (e.g., 0.2): More deterministic and focused output\n- Higher values (e.g., 1.5): More random and creative output\n- Range: [0, 2)",
            "minimum": 0,
            "exclusiveMaximum": 2,
            "default": 0.7,
            "example": 0.7
          },
          "top_p": {
            "type": "number",
            "description": "Nucleus sampling parameter\n\n**Note**:\n- Controls sampling from tokens with cumulative probability\n- e.g., 0.8 means sampling from tokens with top 80% cumulative probability\n- Range: (0, 1]\n\n**Suggestion**: Do not adjust both temperature and top_p simultaneously",
            "exclusiveMinimum": 0,
            "maximum": 1,
            "default": 0.8,
            "example": 0.8
          },
          "top_k": {
            "type": "integer",
            "description": "Top-K sampling parameter, number of candidate tokens\n\n**Note**:\n- Limits sampling to the top K highest probability tokens\n- When value >100, top_k is disabled and only top_p takes effect\n- Smaller values make output more focused",
            "minimum": 0,
            "default": 20,
            "example": 20
          },
          "max_tokens": {
            "type": "integer",
            "description": "Maximum number of output tokens\n\n**Note**:\n- Too small value may cause truncated response\n- Does not limit thinking chain length\n- If max tokens is reached, finish_reason will be \"length\", otherwise \"stop\"",
            "minimum": 1,
            "example": 2000
          },
          "presence_penalty": {
            "type": "number",
            "description": "Presence penalty, positive values reduce repetition\n\n**Note**:\n- Positive values penalize new tokens based on whether they appear in the text, increasing likelihood of discussing new topics",
            "minimum": -2.0,
            "maximum": 2.0,
            "default": 1.5,
            "example": 1.5
          },
          "frequency_penalty": {
            "type": "number",
            "description": "Frequency penalty\n\n**Note**:\n- Positive values penalize new tokens based on their frequency in the text, decreasing likelihood of repeating same phrases verbatim",
            "minimum": -2.0,
            "maximum": 2.0,
            "default": 0.0,
            "example": 0.0
          },
          "seed": {
            "type": "integer",
            "description": "Random seed for reproducible results\n\n**Note**:\n- Range: [0, 2^31-1]\n- Setting the same seed value makes output more reproducible",
            "minimum": 0,
            "maximum": 2147483647,
            "example": 1234
          },
          "stop": {
            "oneOf": [
              { "type": "string", "description": "Single stop word" },
              { "type": "array", "description": "List of stop words", "items": { "type": "string" } }
            ],
            "description": "Stop sequences, generation stops when triggered\n\n**Note**:\n- The stop sequences themselves will not be included in the output"
          },
          "n": {
            "type": "integer",
            "description": "Number of completions to generate\n\n**Note**:\n- Range: 1-4\n- Only available in non-thinking mode\n- When temperature is very close to 0, only 1 result can be returned",
            "minimum": 1,
            "maximum": 4,
            "default": 1,
            "example": 1
          },
          "enable_thinking": {
            "type": "boolean",
            "description": "Whether to enable thinking mode\n\n**Note**:\n- When enabled, thinking content is returned via reasoning_content\n- Forced off in non-streaming mode\n- In thinking mode, the model performs deep reasoning before giving the final answer\n- When not set, default behavior is determined by the upstream model",
            "example": true
          },
          "enable_search": {
            "type": "boolean",
            "description": "Whether to enable web search\n\n**Note**:\n- When enabled, the model will perform web searches as needed to enhance answers\n- Increases token consumption\n- Search costs vary by search strategy",
            "default": false,
            "example": false
          },
          "search_options": {
            "type": "object",
            "description": "Web search options, only effective when enable_search=true",
            "properties": {
              "forced_search": {
                "type": "boolean",
                "description": "Whether to force search\n\n**Note**:\n- Only effective when enable_search=true\n- When true, search is always performed regardless of model judgment",
                "default": false
              },
              "search_strategy": {
                "type": "string",
                "description": "Search strategy\n\n**Note**:\n- `turbo`: Fast search, 30 UC/request\n- `max`: Deep search, 40 UC/request\n- `agent`: Agent search, 40 UC/request, only for qwen3-max series",
                "enum": ["turbo", "max", "agent"],
                "default": "turbo",
                "example": "turbo"
              },
              "enable_search_extension": {
                "type": "boolean",
                "description": "Whether to enable vertical domain search\n\n**Note**:\n- Only effective when enable_search=true\n- Enables deeper search in specific vertical domains",
                "default": false
              }
            }
          },
          "tools": {
            "type": "array",
            "description": "List of tools for Tool Use or Function Calling\n\n**Note**:\n- Each tool must include a type\n- The function structure must include name, description, and parameters",
            "items": { "$ref": "#/components/schemas/Tool" }
          },
          "tool_choice": {
            "oneOf": [
              { "type": "string", "enum": ["auto", "none"] },
              { "type": "object", "description": "Specified tool object" }
            ],
            "description": "Tool selection strategy\n\n**Note**:\n- `auto`: Model decides whether to call tools\n- `none`: Do not call tools\n- Can also specify a specific tool\n- **Warning**: Thinking mode does not support forced tool specification",
            "default": "auto"
          },
          "parallel_tool_calls": {
            "type": "boolean",
            "description": "Whether to enable parallel tool calls\n\n**Note**:\n- When true, the model can call multiple tools simultaneously in one response",
            "default": false,
            "example": false
          },
          "response_format": {
            "type": "object",
            "description": "Output format settings\n\n**Note**:\n- Supports text, json_object, and json_schema formats\n- When using json_object or json_schema, explicitly guide the model to output JSON in your prompt\n- Default: {\"type\": \"text\"}",
            "properties": {
              "type": {
                "type": "string",
                "enum": ["text", "json_object", "json_schema"],
                "description": "Output format type",
                "default": "text"
              }
            }
          },
          "logprobs": {
            "type": "boolean",
            "description": "Whether to return token log probabilities",
            "default": false,
            "example": false
          },
          "top_logprobs": {
            "type": "integer",
            "description": "Number of candidate token probabilities\n\n**Note**:\n- Only effective when logprobs=true\n- Range: [0, 5]",
            "minimum": 0,
            "maximum": 5,
            "default": 0,
            "example": 0
          }
        }
      },
      "Message": {
        "type": "object",
        "required": ["role", "content"],
        "properties": {
          "role": {
            "type": "string",
            "description": "Message role\n\n- `user`: User message\n- `assistant`: AI assistant message\n- `system`: System prompt\n- `tool`: Tool result",
            "enum": ["user", "assistant", "system", "tool"],
            "example": "user"
          },
          "content": {
            "oneOf": [
              { "type": "string", "description": "Plain text message content", "example": "Please introduce yourself" },
              { "type": "array", "description": "Multimodal message content", "items": { "$ref": "#/components/schemas/ContentPart" } }
            ],
            "description": "Message content. Supports two formats:\n\n**1. Plain text string**\n\n**2. Object array** (supports multimodal input)"
          }
        }
      },
      "ContentPart": {
        "oneOf": [
          { "$ref": "#/components/schemas/TextContent" },
          { "$ref": "#/components/schemas/ImageContent" }
        ]
      },
      "TextContent": {
        "title": "Text content",
        "type": "object",
        "required": ["type", "text"],
        "properties": {
          "type": { "type": "string", "enum": ["text"], "description": "Content type" },
          "text": { "type": "string", "description": "Text content", "example": "Please describe this image in detail" }
        }
      },
      "ImageContent": {
        "title": "Image content",
        "type": "object",
        "required": ["type", "image_url"],
        "properties": {
          "type": { "type": "string", "enum": ["image_url"], "description": "Content type" },
          "image_url": {
            "type": "object",
            "required": ["url"],
            "properties": {
              "url": {
                "type": "string",
                "format": "uri",
                "description": "Image URL or Base64 encoding\n\n**Format**:\n- URL: `https://example.com/image.png`\n- Base64: `data:image/<format>;base64,<encoding>`",
                "example": "data:image/png;base64,iVBORw0KGgo..."
              }
            }
          }
        }
      },
      "Tool": {
        "type": "object",
        "required": ["type", "function"],
        "properties": {
          "type": { "type": "string", "enum": ["function"], "description": "Tool type" },
          "function": {
            "type": "object",
            "required": ["name", "description", "parameters"],
            "properties": {
              "name": { "type": "string", "description": "Function name", "pattern": "^[a-zA-Z_][a-zA-Z0-9-_]{0,63}$", "example": "get_weather" },
              "description": { "type": "string", "description": "Function description", "example": "Get weather information for a specified city" },
              "parameters": { "type": "object", "description": "Function parameter definition, a subset of JSON Schema" }
            }
          }
        }
      },
      "ChatCompletionResponse": {
        "type": "object",
        "properties": {
          "id": { "type": "string", "description": "Unique identifier for the chat completion", "example": "cmpl-04ea926191a14749b7f2c7a48a68abc6" },
          "model": { "type": "string", "description": "The model used for completion", "example": "qwen3-max-2026-01-23" },
          "object": { "type": "string", "enum": ["chat.completion"], "description": "Response type", "example": "chat.completion" },
          "created": { "type": "integer", "description": "Unix timestamp when the completion was created", "example": 1698999496 },
          "choices": { "type": "array", "description": "List of completion choices", "items": { "$ref": "#/components/schemas/Choice" } },
          "usage": { "$ref": "#/components/schemas/Usage" }
        }
      },
      "Choice": {
        "type": "object",
        "properties": {
          "index": { "type": "integer", "description": "Index of this choice", "example": 0 },
          "message": { "$ref": "#/components/schemas/AssistantMessage" },
          "finish_reason": {
            "type": "string",
            "description": "Reason why the completion finished\n\n- `stop`: Natural completion\n- `length`: Reached maximum token limit\n- `content_filter`: Content was filtered\n- `tool_calls`: Model requested tool call",
            "enum": ["stop", "length", "content_filter", "tool_calls"],
            "example": "stop"
          }
        }
      },
      "AssistantMessage": {
        "type": "object",
        "properties": {
          "role": { "type": "string", "enum": ["assistant"], "description": "Role of the message sender", "example": "assistant" },
          "content": { "type": "string", "description": "AI's response content", "example": "Hello! How can I help you?" },
          "reasoning_content": {
            "type": "string",
            "description": "Reasoning process content (only returned when enable_thinking=true)\n\n**Note**:\n- Shows the model's thinking and reasoning process\n- Helps understand how the model arrives at its final answer\n- Only returned when thinking mode is enabled",
            "example": "The user is asking a math problem, I need to analyze the inflow and outflow rates first..."
          }
        }
      },
      "Usage": {
        "type": "object",
        "description": "Token usage statistics\n\n**Pricing**:\n- Input tokens: 20 UC / 1K tokens\n- Output tokens (including thinking tokens): 60 UC / 1K tokens",
        "properties": {
          "prompt_tokens": { "type": "integer", "description": "Number of tokens in the input", "example": 8 },
          "completion_tokens": { "type": "integer", "description": "Number of tokens in the output (including thinking tokens)", "example": 292 },
          "total_tokens": { "type": "integer", "description": "Total number of tokens used", "example": 300 },
          "prompt_tokens_details": {
            "type": "object",
            "description": "Detailed breakdown of input tokens",
            "properties": {
              "cached_tokens": { "type": "integer", "description": "Number of cached tokens", "example": 8 }
            }
          }
        }
      },
      "ErrorResponse": {
        "type": "object",
        "properties": {
          "error": {
            "type": "object",
            "properties": {
              "code": { "type": "integer", "description": "HTTP status error code" },
              "message": { "type": "string", "description": "Error message" },
              "type": { "type": "string", "description": "Error type" },
              "param": { "type": "string", "description": "Related parameter name" },
              "fallback_suggestion": { "type": "string", "description": "Suggestion for handling the error" }
            }
          }
        }
      }
    },
    "securitySchemes": {
      "bearerAuth": {
        "type": "http",
        "scheme": "bearer",
        "description": "##All APIs require Bearer Token authentication##\n\n**Get API Key:**\n\nVisit [API Key Management Page](https://evolink.ai/dashboard/keys) to get your API Key\n\n**Add to request header:**\n```\nAuthorization: Bearer YOUR_API_KEY\n```"
      }
    }
  },
  "tags": [
    {
      "name": "Chat Completion",
      "description": "AI chat completion related endpoints"
    }
  ]
}